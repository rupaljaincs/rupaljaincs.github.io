---
layout: post
title: Call for increased collaboration between public and private sector for better AI Governance
subtitle: Private sector does AI innovation, with public watchdogs holding them accountable.
bigimg: /img/path.jpg
tags: [open-source, internship]
---

International law plays an important role in providing states a common vocabulary, ensuring that all stakeholders, be it in local communities, industry, government, and elsewhere speak the same language and have a shared understanding of AI’s capabilities and shortcomings. This is crucial in bridging the gap between the AI understanding between engineers and governments to ensure coordinated progress. Similarly, connecting the private sector in AI with the public sector who are concerned with policy making and AI regulatory bodies promises preservation of societal values in AI governance. 

**International law is technology neutral**. This means that its rules and principles apply to old and new technologies. For example, the International Court of Justice affirmed that the prohibition on the use of force and international humanitarian law apply to all kinds of weapons, irrespective of the technology behind them.

Many governments around the world released strategies and action plans that outline how and why they plan on putting AI development on their political agendas. This generally involves expressing their willingness to invest in STEM education and research bodies, making recommendations on the application of AI in their various national industries etc. 

Why is it important? Because the cost of misunderstanding is too dire to ignore. Such as in weapons automatization or the criminal justice system. Embedding technologists in government will help boost digital literacy and move AI governance from theory to practice.

Two approaches for bridging that gap:
- **Multidisciplinary**: ‘Bilinguals’ in Governance – those with domain and AI expertise, in areas such as health, could be very valuable.
- **Interdisciplinary**: Synthesis of social science research and tech development

**Use-case**: Automobile Industry’s requirements for crash testing reporting for example  - human interaction be part of the engineering process.

Also, I advocate against forming new oversight agencies to approve, monitor and impose penalties for AI use/misuse, that would only add to more bureaucratic red tape and overkill. Instead, we need to bridge and plug existing networks. 

## Some open questions:

- Can we direct governments (and thus, public sector companies) to only procure goods that are evaluated and approved as “AI safe”? How do we establish what an “AI safe good” even means? Even once we do, will it be feasible to regulate the market flow? Despite decades of Treaty on the Non-Proliferation of Nuclear Weapons (US-led) regulation, it has colossally failed to control the ability of nations to acquire enriched Uranium and build their own bombs.
- How do we enhance political and public understanding of the societal impacts of AI and the call for better governance and shape national narratives? In the simplest yet compelling ways possible. I see the impact of digital art and human centered design applications here.
- Perhaps, designing an entirely new academic discipline at the nexus of Computer Science and Social Science could propel us?
- We need global incentives for independent agencies to foster just use of AI.




